{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZIYUNCHEN/SentimentAnalysis/blob/jesse/SentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "_uuid": "839fc1317e1b7253241839bbfa2d40303c53a3f1",
        "id": "CDx4CA09GZG8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## General information\n",
        "\n",
        "In this kernel I'll work with data from Movie Review Sentiment Analysis Playground Competition.\n",
        "\n",
        "This dataset is interesting for NLP researching. Sentences from original dataset were split in separate phrases and each of them has a sentiment label. Also a lot of phrases are really short which makes classifying them quite challenging. Let's try!"
      ]
    },
    {
      "metadata": {
        "id": "Vq4S5_HIGpBc",
        "colab_type": "code",
        "outputId": "1cfaf862-e867-4d1d-c231-436c29b8e144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install lightgbm wordcloud\n",
        "!pip install -q pydot\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.6/dist-packages (2.2.2)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from lightgbm) (0.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.14.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lightgbm) (1.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "9K7leB0DGZG9",
        "colab_type": "code",
        "outputId": "8fed8bb2-17cb-406b-e1e3-1f41347b561a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import datetime\n",
        "import lightgbm as lgb\n",
        "from scipy import stats\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.util import ngrams\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "pd.set_option('max_colwidth',400)\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "B37JjB-XGrY0",
        "colab_type": "code",
        "outputId": "54caada4-05d1-48cb-c25b-0357831003ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/train.tsv\n",
        "!wget https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/test.tsv\n",
        "!wget https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/sampleSubmission.csv"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-11-11 23:32:59--  https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/train.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8481022 (8.1M) [text/plain]\n",
            "Saving to: ‘train.tsv.3’\n",
            "\n",
            "train.tsv.3         100%[===================>]   8.09M  21.7MB/s    in 0.4s    \n",
            "\n",
            "2018-11-11 23:32:59 (21.7 MB/s) - ‘train.tsv.3’ saved [8481022/8481022]\n",
            "\n",
            "--2018-11-11 23:33:00--  https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/test.tsv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3367149 (3.2M) [text/plain]\n",
            "Saving to: ‘test.tsv.3’\n",
            "\n",
            "test.tsv.3          100%[===================>]   3.21M  13.5MB/s    in 0.2s    \n",
            "\n",
            "2018-11-11 23:33:00 (13.5 MB/s) - ‘test.tsv.3’ saved [3367149/3367149]\n",
            "\n",
            "--2018-11-11 23:33:00--  https://raw.githubusercontent.com/MangoHaha/DeepLearning/master/dataset/sampleSubmission.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 596647 (583K) [text/plain]\n",
            "Saving to: ‘sampleSubmission.csv.3’\n",
            "\n",
            "sampleSubmission.cs 100%[===================>] 582.66K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-11-11 23:33:01 (5.02 MB/s) - ‘sampleSubmission.csv.3’ saved [596647/596647]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "scrolled": true,
        "id": "Qp7EQ0TrGZHB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train = pd.read_csv('./train.tsv', sep=\"\\t\")\n",
        "test = pd.read_csv('./test.tsv', sep=\"\\t\")\n",
        "sub = pd.read_csv('./sampleSubmission.csv', sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f9b8d8423bb09068cb168b67f4756ee8b250fc8c",
        "id": "xBg-49HYGZHE",
        "colab_type": "code",
        "outputId": "46bd4885-931a-4976-e3f6-2a71956c6381",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        }
      },
      "cell_type": "code",
      "source": [
        "train.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId  \\\n",
              "0         1           1   \n",
              "1         2           1   \n",
              "2         3           1   \n",
              "3         4           1   \n",
              "4         5           1   \n",
              "5         6           1   \n",
              "6         7           1   \n",
              "7         8           1   \n",
              "8         9           1   \n",
              "9        10           1   \n",
              "\n",
              "                                                                                                                                                                                         Phrase  \\\n",
              "0  A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .   \n",
              "1                                                                                                                 A series of escapades demonstrating the adage that what is good for the goose   \n",
              "2                                                                                                                                                                                      A series   \n",
              "3                                                                                                                                                                                             A   \n",
              "4                                                                                                                                                                                        series   \n",
              "5                                                                                                                          of escapades demonstrating the adage that what is good for the goose   \n",
              "6                                                                                                                                                                                            of   \n",
              "7                                                                                                                             escapades demonstrating the adage that what is good for the goose   \n",
              "8                                                                                                                                                                                     escapades   \n",
              "9                                                                                                                                       demonstrating the adage that what is good for the goose   \n",
              "\n",
              "   Sentiment  \n",
              "0          1  \n",
              "1          2  \n",
              "2          2  \n",
              "3          2  \n",
              "4          2  \n",
              "5          2  \n",
              "6          2  \n",
              "7          2  \n",
              "8          2  \n",
              "9          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e94f8371d8be87186f23ecc75178480d3d96bd78",
        "id": "AE2gjhWFGZHL",
        "colab_type": "code",
        "outputId": "79d02a70-8b21-4ec2-cf9f-da95df22f089",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        }
      },
      "cell_type": "code",
      "source": [
        "train.loc[train.SentenceId == 2]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>64</td>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining independent is worth seeking .</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>This quiet , introspective and entertaining independent</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>This</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>67</td>\n",
              "      <td>2</td>\n",
              "      <td>quiet , introspective and entertaining independent</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>68</td>\n",
              "      <td>2</td>\n",
              "      <td>quiet , introspective and entertaining</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>69</td>\n",
              "      <td>2</td>\n",
              "      <td>quiet</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>70</td>\n",
              "      <td>2</td>\n",
              "      <td>, introspective and entertaining</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>71</td>\n",
              "      <td>2</td>\n",
              "      <td>introspective and entertaining</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>72</td>\n",
              "      <td>2</td>\n",
              "      <td>introspective and</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>73</td>\n",
              "      <td>2</td>\n",
              "      <td>introspective</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>74</td>\n",
              "      <td>2</td>\n",
              "      <td>and</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>75</td>\n",
              "      <td>2</td>\n",
              "      <td>entertaining</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>76</td>\n",
              "      <td>2</td>\n",
              "      <td>independent</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>is worth seeking .</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>78</td>\n",
              "      <td>2</td>\n",
              "      <td>is worth seeking</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>79</td>\n",
              "      <td>2</td>\n",
              "      <td>is worth</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>80</td>\n",
              "      <td>2</td>\n",
              "      <td>worth</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>81</td>\n",
              "      <td>2</td>\n",
              "      <td>seeking</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    PhraseId  SentenceId  \\\n",
              "63        64           2   \n",
              "64        65           2   \n",
              "65        66           2   \n",
              "66        67           2   \n",
              "67        68           2   \n",
              "68        69           2   \n",
              "69        70           2   \n",
              "70        71           2   \n",
              "71        72           2   \n",
              "72        73           2   \n",
              "73        74           2   \n",
              "74        75           2   \n",
              "75        76           2   \n",
              "76        77           2   \n",
              "77        78           2   \n",
              "78        79           2   \n",
              "79        80           2   \n",
              "80        81           2   \n",
              "\n",
              "                                                                        Phrase  \\\n",
              "63  This quiet , introspective and entertaining independent is worth seeking .   \n",
              "64                     This quiet , introspective and entertaining independent   \n",
              "65                                                                        This   \n",
              "66                          quiet , introspective and entertaining independent   \n",
              "67                                      quiet , introspective and entertaining   \n",
              "68                                                                       quiet   \n",
              "69                                            , introspective and entertaining   \n",
              "70                                              introspective and entertaining   \n",
              "71                                                           introspective and   \n",
              "72                                                               introspective   \n",
              "73                                                                         and   \n",
              "74                                                                entertaining   \n",
              "75                                                                 independent   \n",
              "76                                                          is worth seeking .   \n",
              "77                                                            is worth seeking   \n",
              "78                                                                    is worth   \n",
              "79                                                                       worth   \n",
              "80                                                                     seeking   \n",
              "\n",
              "    Sentiment  \n",
              "63          4  \n",
              "64          3  \n",
              "65          2  \n",
              "66          4  \n",
              "67          3  \n",
              "68          2  \n",
              "69          3  \n",
              "70          3  \n",
              "71          3  \n",
              "72          2  \n",
              "73          2  \n",
              "74          4  \n",
              "75          2  \n",
              "76          3  \n",
              "77          4  \n",
              "78          2  \n",
              "79          2  \n",
              "80          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f75e24b86e3aeb7477fa1cd69789992233420b1",
        "id": "WUMsIz3fGZHN",
        "colab_type": "code",
        "outputId": "78f4b990-32a5-45b1-a7cd-13ed13ddd863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\n",
        "print('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average count of phrases per sentence in train is 18.\n",
            "Average count of phrases per sentence in test is 20.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca3148a6bbdd0f71e4909feb5dca874fa6d64a2e",
        "id": "82tj6yniGZHQ",
        "colab_type": "code",
        "outputId": "a2e4d0b9-ebbb-4d68-e9ec-f4d0788c6a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\n",
        "print('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of phrases in train: 156060. Number of sentences in train: 8529.\n",
            "Number of phrases in test: 66292. Number of sentences in test: 3310.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bef53d8f659b78e3fc6b1ed07025591d55b7c128",
        "id": "jlqd1ZRtGZHU",
        "colab_type": "code",
        "outputId": "bcba8a80-7914-4d0f-88df-9478197cf906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\n",
        "print('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average word length of phrases in train is 7.\n",
            "Average word length of phrases in test is 7.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "9ee27697b41dfdca9cdb482bfc85cfd3d63ae6e2",
        "id": "hWLkVYcCGZHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see than sentences were split in 18-20 phrases at average and a lot of phrases contain each other. Sometimes one word or even one punctuation mark influences the sentiment"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d1efbed65f250d37472544f4fe37cb6fd13e183",
        "collapsed": true,
        "id": "LNCjPzRnGZHZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's see for example most common trigrams for positive phrases"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a128ab1d36eef6ec47161705a2b1094b830fe10",
        "id": "4khms7mwGZHb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
        "text_trigrams = [i for i in ngrams(text.split(), 3)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b5c8533c5861794d44b53dbb2e5ef764e5e88119",
        "id": "WiF4y7O_GZHe",
        "colab_type": "code",
        "outputId": "60c165b6-f4fc-40cf-bfe9-9d5c35ae8819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "Counter(text_trigrams).most_common(30)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(('one', 'of', 'the'), 199),\n",
              " (('of', 'the', 'year'), 103),\n",
              " (('.', 'is', 'a'), 87),\n",
              " (('of', 'the', 'best'), 80),\n",
              " (('of', 'the', 'most'), 70),\n",
              " (('is', 'one', 'of'), 50),\n",
              " (('One', 'of', 'the'), 43),\n",
              " ((',', 'and', 'the'), 40),\n",
              " (('the', 'year', \"'s\"), 38),\n",
              " (('It', \"'s\", 'a'), 38),\n",
              " (('it', \"'s\", 'a'), 37),\n",
              " (('.', \"'s\", 'a'), 37),\n",
              " (('a', 'movie', 'that'), 35),\n",
              " (('the', 'edge', 'of'), 34),\n",
              " (('the', 'kind', 'of'), 33),\n",
              " (('of', 'your', 'seat'), 33),\n",
              " (('the', 'film', 'is'), 31),\n",
              " ((',', 'this', 'is'), 31),\n",
              " (('the', 'film', \"'s\"), 31),\n",
              " ((',', 'the', 'film'), 30),\n",
              " (('film', 'that', 'is'), 30),\n",
              " (('as', 'one', 'of'), 30),\n",
              " (('edge', 'of', 'your'), 29),\n",
              " ((',', 'it', \"'s\"), 27),\n",
              " (('a', 'film', 'that'), 27),\n",
              " (('as', 'well', 'as'), 27),\n",
              " ((',', 'funny', ','), 25),\n",
              " ((',', 'but', 'it'), 23),\n",
              " (('films', 'of', 'the'), 23),\n",
              " (('some', 'of', 'the'), 23)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "402c0b58fe43a680bea33e1813012bec5c16cb55",
        "id": "zr1gESsOGZHm",
        "colab_type": "code",
        "outputId": "7ca7310c-e15d-4c62-aa62-df1895d38e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "cell_type": "code",
      "source": [
        "text = ' '.join(train.loc[train.Sentiment == 4, 'Phrase'].values)\n",
        "text = [i for i in text.split() if i not in stopwords.words('english')]\n",
        "text_trigrams = [i for i in ngrams(text, 3)]\n",
        "Counter(text_trigrams).most_common(30)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[((',', 'funny', ','), 33),\n",
              " (('one', 'year', \"'s\"), 28),\n",
              " (('year', \"'s\", 'best'), 26),\n",
              " (('movies', 'ever', 'made'), 19),\n",
              " ((',', 'solid', 'cast'), 19),\n",
              " (('solid', 'cast', ','), 18),\n",
              " ((\"'ve\", 'ever', 'seen'), 16),\n",
              " (('.', 'It', \"'s\"), 16),\n",
              " ((',', 'making', 'one'), 15),\n",
              " (('best', 'films', 'year'), 15),\n",
              " ((',', 'touching', ','), 15),\n",
              " (('exquisite', 'acting', ','), 15),\n",
              " (('acting', ',', 'inventive'), 14),\n",
              " ((',', 'inventive', 'screenplay'), 14),\n",
              " (('jaw-dropping', 'action', 'sequences'), 14),\n",
              " (('good', 'acting', ','), 14),\n",
              " ((\"'s\", 'best', 'films'), 14),\n",
              " (('I', \"'ve\", 'seen'), 14),\n",
              " (('funny', ',', 'even'), 14),\n",
              " (('best', 'war', 'movies'), 13),\n",
              " (('purely', 'enjoyable', 'satisfying'), 13),\n",
              " (('funny', ',', 'touching'), 13),\n",
              " ((',', 'smart', ','), 13),\n",
              " (('inventive', 'screenplay', ','), 13),\n",
              " (('funniest', 'jokes', 'movie'), 13),\n",
              " (('action', 'sequences', ','), 13),\n",
              " (('sequences', ',', 'striking'), 13),\n",
              " ((',', 'striking', 'villains'), 13),\n",
              " (('exquisite', 'motion', 'picture'), 13),\n",
              " (('war', 'movies', 'ever'), 12)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f59db6ed32fafb024728fd95b96157f278682a74",
        "id": "KZBtEbGKGZHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The results show the main problem with this dataset: there are to many common words due to sentenced splitted in phrases. As a result stopwords shouldn't be removed from text."
      ]
    },
    {
      "metadata": {
        "_uuid": "10dc8fc8d535ef492a3dab1b85b4052feec756ee",
        "id": "4qTvvMwYGZHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Thoughts on feature processing and engineering"
      ]
    },
    {
      "metadata": {
        "_uuid": "d5d018a694d10cbd9e8e89c26d5227fdb9cf8c0b",
        "id": "lUzx6-B0GZHr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n",
        "- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n",
        "- puntuation could be important, so it should be used;\n",
        "- ngrams are necessary to get the most info from data;\n",
        "- using features like word count or sentence length won't be useful;"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bd603ad818970c3c8c6db5e430a6cb8ae8eafbd5",
        "id": "XEQTtoabGZHs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7ebc937fd5ec811bc5c529ff416180fe338d073d",
        "id": "nbSy71IgGZHw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\n",
        "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\n",
        "vectorizer.fit(full_text)\n",
        "train_vectorized = vectorizer.transform(train['Phrase'])\n",
        "test_vectorized = vectorizer.transform(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d0bb4539b1e0b5f8439878a967ce7b5ece23f60",
        "id": "Wq4b0ssAGZH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = train['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75a7acfd815fb391cad92eed8d2f13e5c9801de8",
        "id": "8-9VdgD0GZH4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression()\n",
        "ovr = OneVsRestClassifier(logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a0bf05b880e0d05da378b753394e5a631753e82",
        "id": "RGIQrR6GGZH7",
        "colab_type": "code",
        "outputId": "11a78405-bcda-4a19-ede3-436c93fe83f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "ovr.fit(train_vectorized, y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14 s, sys: 55.6 ms, total: 14.1 s\n",
            "Wall time: 14.1 s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False),\n",
              "          n_jobs=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5946485410c25ae5033bf39e29f49f606ea87bfe",
        "id": "EM7TTGtFGZH8",
        "colab_type": "code",
        "outputId": "9706ff84-a08a-4aa5-9e33-8998b0358c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "scores = cross_val_score(ovr, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation mean accuracy 56.55%, std 0.07.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "755c2b04f7bf8e86e9dc6242f392a575c61a052c",
        "id": "4mA7KtH8GZIA",
        "colab_type": "code",
        "outputId": "1a5b3fd0-d6f6-4f0d-d7aa-6456e87371a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "svc = LinearSVC(dual=False)\n",
        "scores = cross_val_score(svc, train_vectorized, y, scoring='accuracy', n_jobs=-1, cv=3)\n",
        "print('Cross-validation mean accuracy {0:.2f}%, std {1:.2f}.'.format(np.mean(scores) * 100, np.std(scores) * 100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross-validation mean accuracy 56.51%, std 0.68.\n",
            "CPU times: user 351 ms, sys: 108 ms, total: 459 ms\n",
            "Wall time: 37.7 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55a5845de3412244cfa9f0a11392e50c76d78184",
        "id": "2ZLraC0oGZID",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ovr.fit(train_vectorized, y);\n",
        "svc.fit(train_vectorized, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "398461363c7a395e2a982e07e8ac6fccaee139c1",
        "id": "D-qyZxyVGZIG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Deep learning\n",
        "And now let's try DL. DL should work better for text classification with multiple layers. I use an architecture similar to those which were used in toxic competition."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eb29ec027df57f6597dbef976645dc8d151e1618",
        "id": "AwnzDD_0GZIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e250f6d-8f8a-402f-e45c-b658bd38ceaf"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
        "from keras.models import Model, load_model\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "from keras import backend as K\n",
        "from keras.engine import InputSpec, Layer\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "AfEpa3vhJHfk",
        "colab_type": "code",
        "outputId": "377d521d-582f-4543-c943-b9ed6f37096c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install pydrive\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls \"/content/drive/My Drive\"\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "'alaska pics'\t\t\t        life\n",
            " Citi\t\t\t\t        Photos\n",
            "'Colab Notebooks'\t\t       'SICO RESEARCH DATABASE.gsheet'\n",
            " crawl-300d-2M.vec\t\t        tensor_tutorial.ipynb\n",
            " Cryptos\t\t\t        test2.mov\n",
            "'Deep Learning Project Proposal.gdoc'   unknown\n",
            "'GSPGC (1).ipynb'\t\t        Untitled0.ipynb\n",
            " HW1-zc2243-lc2928.zip\t\t       'Untitled document.gdoc'\n",
            " imdb.npz\t\t\t       'Ziyun Chen 0909-1.pdf'\n",
            "'imdb sentiment lstm.ipynb'\t        视频.MOV\n",
            "'Jinyang.Li  0909.pdf'\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "PoAdDzbVK0GJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!ls \"/content/drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a2881c29f82578b4a373b52d2c7b96a2e73bfd80",
        "id": "TP2z0XJhGZIL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tk = Tokenizer(lower = True, filters='')\n",
        "tk.fit_on_texts(full_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1724669c7ca010d1bbf75e200211afdead768d1f",
        "id": "10YBW01vGZIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_tokenized = tk.texts_to_sequences(train['Phrase'])\n",
        "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lEkGLjrQiA5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "7b59c064-ec34-4198-bb67-ea9dbaf6d21f"
      },
      "cell_type": "code",
      "source": [
        "train_tokenized[1:10]\n",
        "train[1:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PhraseId</th>\n",
              "      <th>SentenceId</th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>A series of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>A series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>series</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>of escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>of</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>escapades</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>demonstrating the adage that what is good for the goose</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PhraseId  SentenceId  \\\n",
              "1         2           1   \n",
              "2         3           1   \n",
              "3         4           1   \n",
              "4         5           1   \n",
              "5         6           1   \n",
              "6         7           1   \n",
              "7         8           1   \n",
              "8         9           1   \n",
              "9        10           1   \n",
              "\n",
              "                                                                          Phrase  \\\n",
              "1  A series of escapades demonstrating the adage that what is good for the goose   \n",
              "2                                                                       A series   \n",
              "3                                                                              A   \n",
              "4                                                                         series   \n",
              "5           of escapades demonstrating the adage that what is good for the goose   \n",
              "6                                                                             of   \n",
              "7              escapades demonstrating the adage that what is good for the goose   \n",
              "8                                                                      escapades   \n",
              "9                        demonstrating the adage that what is good for the goose   \n",
              "\n",
              "   Sentiment  \n",
              "1          2  \n",
              "2          2  \n",
              "3          2  \n",
              "4          2  \n",
              "5          2  \n",
              "6          2  \n",
              "7          2  \n",
              "8          2  \n",
              "9          2  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bcb80cf8a59ca779a0be1ab235a1e9da2f4b175b",
        "id": "b6bErvirGZIP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        },
        "outputId": "800e2b9a-1958-4948-ac92-5009a76c0dc0"
      },
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
        "\n",
        "X_train[1:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            3,   308,     4, 18031,  7598,     1,  8322,    11,    55,\n",
              "           10,    51,    15,     1,  4660],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     3,   308],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     3],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,   308],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     4, 18031,  7598,     1,  8322,    11,    55,\n",
              "           10,    51,    15,     1,  4660],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     4],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0, 18031,  7598,     1,  8322,    11,    55,\n",
              "           10,    51,    15,     1,  4660],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0, 18031],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,  7598,     1,  8322,    11,    55,\n",
              "           10,    51,    15,     1,  4660]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dfd0b8fa2c79bfa206d2fe8e35fbec444418f5c",
        "id": "x2VpwGsgGZIS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_path = \"/content/drive/My Drive/crawl-300d-2M.vec\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d74c2aa2f13e8544f045e5644d5bac70e248a8bd",
        "id": "pXkIUeQ5GZIU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embed_size = 300\n",
        "max_features = 30000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cdb522c23b75331481145b789cf127b39d47eaa1",
        "id": "oiQRmxWuGZIX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n",
        "\n",
        "word_index = tk.word_index\n",
        "nb_words = min(max_features, len(word_index))\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_features: continue\n",
        "    embedding_vector = embedding_index.get(word)\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "365c0d607d55a78c5890268b9c168eb12a211855",
        "id": "4AlRADppGZIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "67a3bfb1-1203-4bea-8442-ee71f7a6fd9a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))\n",
        "y_ohe[1:10]\n",
        "\n",
        "embedding_matrix[1:10]\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.0231    ,  0.017     ,  0.0157    , ...,  0.0744    ,\n",
              "        -0.1118    ,  0.0963    ],\n",
              "       [-0.0282    , -0.0557    , -0.0451    , ..., -0.037     ,\n",
              "        -0.0725    , -0.0042    ],\n",
              "       [ 0.0064    ,  0.0333    ,  0.0225    , ..., -0.0825    ,\n",
              "         0.0519    , -0.0796    ],\n",
              "       ...,\n",
              "       [-0.0849    , -0.0582    , -0.0321    , ...,  0.0032    ,\n",
              "        -0.0237    , -0.0366    ],\n",
              "       [-0.0347    , -0.028     ,  0.0704    , ..., -0.0445    ,\n",
              "         0.037     , -0.1166    ],\n",
              "       [-0.0037    ,  0.15099999,  0.0568    , ..., -0.0618    ,\n",
              "         0.1585    , -0.0466    ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "id": "4A_dt4EbBLvK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import plot_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "594273c2d887315d083a35a5ffd7c2dd40c2ebb6",
        "id": "_ohFDzosGZId",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "    \n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n",
        "    max_pool3_gru = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
        "    \n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n",
        "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    plot_model(model, to_file='model_summary.png',show_shapes=True, show_layer_names=True)\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A4IZ9YmXFZin",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* The sigmoid function takes a real-valued number and squashes it to a value in the range between 0 and 1. The function has been in frequent use historically due to its nice interpretation as the firing rate of a neuron: 0 for not firing or 1 for firing. But the non-linearity of the sigmoid has recently fallen out of favour because its activations can easily saturate at either tail of 0 or 1, where gradients are almost zero and the information flow would be cut. What is more is that its output is not zero- centered, which could introduce undesirable zig-zagging dynamics in the gradient updates for the connection weights in training. Thus, the tanh function is often more preferred in practice as its output range is zero-centered, [-1, 1] instead of [0, 1]. The ReLU function has also become popular lately. Its activation is simply thresholded at zero when the input is less than 0. Compared with the sigmoid function and the tanh function, ReLU is easy to compute, fast to converge in training and yields equal or better performance in neural networks.5\n",
        "\n",
        "Generally, softmax is used in the final layer of neural networks for final classification in feedforward neural networks."
      ]
    },
    {
      "metadata": {
        "_uuid": "95f52b1f6de4e939c8d21e3525503912282fbd47",
        "id": "fdY5hCa9GZIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An attempt at ensemble:"
      ]
    },
    {
      "metadata": {
        "id": "FJeNBzSjBUUM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2f482c92-d49e-4ab1-d741-bf3d76aaf633"
      },
      "cell_type": "code",
      "source": [
        "!pip install pydot\n",
        "!pip install graphviz\n",
        "!pip install pydotplus"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.2.4)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.6/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from pydotplus) (2.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5eb586c98fb75c25cac099cd03d8233185fdc317",
        "id": "Jy1ApOlMGZIg",
        "colab_type": "code",
        "outputId": "bf6e3ee9-c1d9-4175-b189-790a75320274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1255
        }
      },
      "cell_type": "code",
      "source": [
        "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                 \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\u001b[0m\u001b[1;32m   1862\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1343\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot': 'dot'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# to check the pydot/graphviz installation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pydot.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1866\u001b[0m                     prog=prog)\n\u001b[0;32m-> 1867\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-67225eb9c3bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_dr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-04d494cc7864>\u001b[0m in \u001b[0;36mbuild_model1\u001b[0;34m(lr, lr_d, units, spatial_dr, kernel_size1, kernel_size2, dense_units, dr, conv_size)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary_crossentropy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_summary.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n\u001b[1;32m     41\u001b[0m                         verbose = 1, callbacks = [check_point, early_stop])\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;34m'LR'\u001b[0m \u001b[0mcreates\u001b[0m \u001b[0ma\u001b[0m \u001b[0mhorizontal\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \"\"\"\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rankdir'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrankdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         raise OSError(\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0;34m'`pydot` failed to call GraphViz.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;34m'Please install GraphViz (https://www.graphviz.org/) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             'and ensure that its executables are in the $PATH.')\n",
            "\u001b[0;31mOSError\u001b[0m: `pydot` failed to call GraphViz.Please install GraphViz (https://www.graphviz.org/) and ensure that its executables are in the $PATH."
          ]
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b059392aad7d904adfb8ae151ad2004aa03da30d",
        "id": "udbZnAtUGZIj",
        "colab_type": "code",
        "outputId": "41144ec4-0f85-4b73-ffc0-572c9ee211c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "cell_type": "code",
      "source": [
        "model2 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 128, spatial_dr = 0.5, kernel_size1=3, kernel_size2=2, dense_units=64, dr=0.2, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140454 samples, validate on 15606 samples\n",
            "Epoch 1/20\n",
            "140454/140454 [==============================] - 88s 625us/step - loss: 0.3565 - acc: 0.8401 - val_loss: 0.3381 - val_acc: 0.8451\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.33809, saving model to best_model.hdf5\n",
            "Epoch 2/20\n",
            "140454/140454 [==============================] - 86s 609us/step - loss: 0.3221 - acc: 0.8544 - val_loss: 0.3131 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.33809 to 0.31310, saving model to best_model.hdf5\n",
            "Epoch 3/20\n",
            "140454/140454 [==============================] - 85s 608us/step - loss: 0.3124 - acc: 0.8579 - val_loss: 0.3070 - val_acc: 0.8565\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31310 to 0.30696, saving model to best_model.hdf5\n",
            "Epoch 4/20\n",
            "140454/140454 [==============================] - 86s 612us/step - loss: 0.3038 - acc: 0.8614 - val_loss: 0.3048 - val_acc: 0.8584\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30696 to 0.30484, saving model to best_model.hdf5\n",
            "Epoch 5/20\n",
            "140454/140454 [==============================] - 84s 601us/step - loss: 0.2976 - acc: 0.8646 - val_loss: 0.3012 - val_acc: 0.8608\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.30484 to 0.30123, saving model to best_model.hdf5\n",
            "Epoch 6/20\n",
            "140454/140454 [==============================] - 83s 594us/step - loss: 0.2928 - acc: 0.8665 - val_loss: 0.3010 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.30123 to 0.30099, saving model to best_model.hdf5\n",
            "Epoch 7/20\n",
            "140454/140454 [==============================] - 83s 591us/step - loss: 0.2876 - acc: 0.8688 - val_loss: 0.3023 - val_acc: 0.8583\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30099\n",
            "Epoch 8/20\n",
            "140454/140454 [==============================] - 83s 594us/step - loss: 0.2841 - acc: 0.8711 - val_loss: 0.2995 - val_acc: 0.8619\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.30099 to 0.29951, saving model to best_model.hdf5\n",
            "Epoch 9/20\n",
            "140454/140454 [==============================] - 84s 597us/step - loss: 0.2800 - acc: 0.8723 - val_loss: 0.3001 - val_acc: 0.8623\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.29951\n",
            "Epoch 10/20\n",
            "140454/140454 [==============================] - 86s 610us/step - loss: 0.2772 - acc: 0.8739 - val_loss: 0.3019 - val_acc: 0.8627\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.29951\n",
            "Epoch 11/20\n",
            "140454/140454 [==============================] - 85s 608us/step - loss: 0.2743 - acc: 0.8755 - val_loss: 0.2986 - val_acc: 0.8621\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.29951 to 0.29862, saving model to best_model.hdf5\n",
            "Epoch 12/20\n",
            "140454/140454 [==============================] - 85s 604us/step - loss: 0.2714 - acc: 0.8767 - val_loss: 0.3043 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.29862\n",
            "Epoch 13/20\n",
            "140454/140454 [==============================] - 83s 593us/step - loss: 0.2695 - acc: 0.8777 - val_loss: 0.2997 - val_acc: 0.8618\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29862\n",
            "Epoch 14/20\n",
            "140454/140454 [==============================] - 83s 592us/step - loss: 0.2678 - acc: 0.8790 - val_loss: 0.3016 - val_acc: 0.8624\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29862\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8187e167ce93f0eb69f59cb9d7fedc4637a77cfe",
        "id": "jUBolZU1GZIl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
        "    file_path = \"best_model.hdf5\"\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
        "                                  save_best_only = True, mode = \"min\")\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
        "\n",
        "    inp = Input(shape = (max_len,))\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
        "\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n",
        "    \n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
        "    \n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
        "    \n",
        "    \n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
        "    \n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
        "    \n",
        "    \n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\n",
        "    model = Model(inputs = inp, outputs = x)\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
        "    \n",
        "\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\n",
        "    model = load_model(file_path)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bf90d6d4effebb3c5aa9b666b8e09c9c57d94d3",
        "id": "gm7qZAr6GZIn",
        "colab_type": "code",
        "outputId": "a2eb5f5b-85c8-4efd-c90c-3f26a64bea06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1394
        }
      },
      "cell_type": "code",
      "source": [
        "model3 = build_model2(lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140454 samples, validate on 15606 samples\n",
            "Epoch 1/20\n",
            "140454/140454 [==============================] - 84s 601us/step - loss: 0.5419 - acc: 0.7317 - val_loss: 0.4162 - val_acc: 0.8358\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.41616, saving model to best_model.hdf5\n",
            "Epoch 2/20\n",
            "140454/140454 [==============================] - 77s 549us/step - loss: 0.3939 - acc: 0.8298 - val_loss: 0.3497 - val_acc: 0.8430\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.41616 to 0.34972, saving model to best_model.hdf5\n",
            "Epoch 3/20\n",
            "140454/140454 [==============================] - 72s 511us/step - loss: 0.3583 - acc: 0.8388 - val_loss: 0.3318 - val_acc: 0.8449\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.34972 to 0.33182, saving model to best_model.hdf5\n",
            "Epoch 4/20\n",
            "140454/140454 [==============================] - 69s 493us/step - loss: 0.3441 - acc: 0.8436 - val_loss: 0.3241 - val_acc: 0.8473\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.33182 to 0.32406, saving model to best_model.hdf5\n",
            "Epoch 5/20\n",
            "140454/140454 [==============================] - 71s 503us/step - loss: 0.3361 - acc: 0.8466 - val_loss: 0.3193 - val_acc: 0.8499\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.32406 to 0.31934, saving model to best_model.hdf5\n",
            "Epoch 6/20\n",
            "140454/140454 [==============================] - 71s 506us/step - loss: 0.3312 - acc: 0.8485 - val_loss: 0.3174 - val_acc: 0.8507\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.31934 to 0.31742, saving model to best_model.hdf5\n",
            "Epoch 7/20\n",
            "140454/140454 [==============================] - 72s 513us/step - loss: 0.3268 - acc: 0.8507 - val_loss: 0.3166 - val_acc: 0.8522\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.31742 to 0.31662, saving model to best_model.hdf5\n",
            "Epoch 8/20\n",
            "140454/140454 [==============================] - 72s 515us/step - loss: 0.3229 - acc: 0.8531 - val_loss: 0.3195 - val_acc: 0.8523\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.31662\n",
            "Epoch 9/20\n",
            "140454/140454 [==============================] - 71s 506us/step - loss: 0.3200 - acc: 0.8540 - val_loss: 0.3128 - val_acc: 0.8542\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.31662 to 0.31277, saving model to best_model.hdf5\n",
            "Epoch 10/20\n",
            "140454/140454 [==============================] - 71s 509us/step - loss: 0.3172 - acc: 0.8548 - val_loss: 0.3113 - val_acc: 0.8544\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.31277 to 0.31129, saving model to best_model.hdf5\n",
            "Epoch 11/20\n",
            "140454/140454 [==============================] - 72s 510us/step - loss: 0.3150 - acc: 0.8561 - val_loss: 0.3141 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.31129\n",
            "Epoch 12/20\n",
            "140454/140454 [==============================] - 72s 509us/step - loss: 0.3122 - acc: 0.8570 - val_loss: 0.3099 - val_acc: 0.8553\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.31129 to 0.30990, saving model to best_model.hdf5\n",
            "Epoch 13/20\n",
            "140454/140454 [==============================] - 71s 508us/step - loss: 0.3099 - acc: 0.8583 - val_loss: 0.3088 - val_acc: 0.8561\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.30990 to 0.30883, saving model to best_model.hdf5\n",
            "Epoch 14/20\n",
            "140454/140454 [==============================] - 72s 510us/step - loss: 0.3084 - acc: 0.8592 - val_loss: 0.3083 - val_acc: 0.8571\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.30883 to 0.30832, saving model to best_model.hdf5\n",
            "Epoch 15/20\n",
            "140454/140454 [==============================] - 72s 511us/step - loss: 0.3071 - acc: 0.8597 - val_loss: 0.3092 - val_acc: 0.8569\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.30832\n",
            "Epoch 16/20\n",
            "140454/140454 [==============================] - 72s 516us/step - loss: 0.3048 - acc: 0.8607 - val_loss: 0.3068 - val_acc: 0.8572\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.30832 to 0.30676, saving model to best_model.hdf5\n",
            "Epoch 17/20\n",
            "140454/140454 [==============================] - 72s 512us/step - loss: 0.3035 - acc: 0.8609 - val_loss: 0.3105 - val_acc: 0.8557\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.30676\n",
            "Epoch 18/20\n",
            "140454/140454 [==============================] - 72s 510us/step - loss: 0.3015 - acc: 0.8621 - val_loss: 0.3087 - val_acc: 0.8580\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.30676\n",
            "Epoch 19/20\n",
            "140454/140454 [==============================] - 72s 513us/step - loss: 0.3002 - acc: 0.8628 - val_loss: 0.3061 - val_acc: 0.8587\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.30676 to 0.30612, saving model to best_model.hdf5\n",
            "Epoch 20/20\n",
            "140454/140454 [==============================] - 71s 507us/step - loss: 0.2987 - acc: 0.8628 - val_loss: 0.3072 - val_acc: 0.8576\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.30612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf6d8d367c5adc30e00bbd77c1de70fd52960441",
        "id": "sCmRToDbGZIq",
        "colab_type": "code",
        "outputId": "41c66d0b-410e-4798-ab80-26ce8f7c2914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "cell_type": "code",
      "source": [
        "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140454 samples, validate on 15606 samples\n",
            "Epoch 1/20\n",
            "140454/140454 [==============================] - 79s 559us/step - loss: 0.3737 - acc: 0.8317 - val_loss: 0.3246 - val_acc: 0.8495\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.32462, saving model to best_model.hdf5\n",
            "Epoch 2/20\n",
            "140454/140454 [==============================] - 75s 537us/step - loss: 0.3247 - acc: 0.8531 - val_loss: 0.3166 - val_acc: 0.8525\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.32462 to 0.31660, saving model to best_model.hdf5\n",
            "Epoch 3/20\n",
            "140454/140454 [==============================] - 72s 511us/step - loss: 0.3147 - acc: 0.8566 - val_loss: 0.3099 - val_acc: 0.8560\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31660 to 0.30994, saving model to best_model.hdf5\n",
            "Epoch 4/20\n",
            "140454/140454 [==============================] - 70s 500us/step - loss: 0.3067 - acc: 0.8600 - val_loss: 0.3045 - val_acc: 0.8579\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30994 to 0.30447, saving model to best_model.hdf5\n",
            "Epoch 5/20\n",
            "140454/140454 [==============================] - 70s 502us/step - loss: 0.3001 - acc: 0.8628 - val_loss: 0.3022 - val_acc: 0.8592\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.30447 to 0.30217, saving model to best_model.hdf5\n",
            "Epoch 6/20\n",
            "140454/140454 [==============================] - 71s 504us/step - loss: 0.2954 - acc: 0.8651 - val_loss: 0.3009 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.30217 to 0.30094, saving model to best_model.hdf5\n",
            "Epoch 7/20\n",
            "140454/140454 [==============================] - 71s 508us/step - loss: 0.2911 - acc: 0.8666 - val_loss: 0.3024 - val_acc: 0.8594\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30094\n",
            "Epoch 8/20\n",
            "140454/140454 [==============================] - 70s 500us/step - loss: 0.2872 - acc: 0.8691 - val_loss: 0.3043 - val_acc: 0.8597\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.30094\n",
            "Epoch 9/20\n",
            "140454/140454 [==============================] - 69s 495us/step - loss: 0.2846 - acc: 0.8702 - val_loss: 0.3021 - val_acc: 0.8599\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.30094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b10113439be683bf19930750eaf96328e5b58d42",
        "id": "45uzbiL4GZIs",
        "colab_type": "code",
        "outputId": "bb179272-0f48-456b-997a-2a40d8e17b1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "cell_type": "code",
      "source": [
        "model5 = build_model2(lr = 1e-3, lr_d = 1e-7, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.4, conv_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 140454 samples, validate on 15606 samples\n",
            "Epoch 1/20\n",
            "140454/140454 [==============================] - 83s 588us/step - loss: 0.3797 - acc: 0.8284 - val_loss: 0.3210 - val_acc: 0.8498\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.32102, saving model to best_model.hdf5\n",
            "Epoch 2/20\n",
            "140454/140454 [==============================] - 78s 552us/step - loss: 0.3215 - acc: 0.8550 - val_loss: 0.3153 - val_acc: 0.8547\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.32102 to 0.31526, saving model to best_model.hdf5\n",
            "Epoch 3/20\n",
            "140454/140454 [==============================] - 73s 520us/step - loss: 0.3081 - acc: 0.8597 - val_loss: 0.3095 - val_acc: 0.8559\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.31526 to 0.30948, saving model to best_model.hdf5\n",
            "Epoch 4/20\n",
            "140454/140454 [==============================] - 72s 515us/step - loss: 0.2978 - acc: 0.8636 - val_loss: 0.3016 - val_acc: 0.8602\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.30948 to 0.30160, saving model to best_model.hdf5\n",
            "Epoch 5/20\n",
            "140454/140454 [==============================] - 72s 515us/step - loss: 0.2902 - acc: 0.8672 - val_loss: 0.3033 - val_acc: 0.8603\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.30160\n",
            "Epoch 6/20\n",
            "140454/140454 [==============================] - 72s 516us/step - loss: 0.2836 - acc: 0.8702 - val_loss: 0.3015 - val_acc: 0.8614\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.30160 to 0.30151, saving model to best_model.hdf5\n",
            "Epoch 7/20\n",
            "140454/140454 [==============================] - 72s 513us/step - loss: 0.2785 - acc: 0.8727 - val_loss: 0.3050 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.30151\n",
            "Epoch 8/20\n",
            "140454/140454 [==============================] - 74s 529us/step - loss: 0.2733 - acc: 0.8753 - val_loss: 0.3002 - val_acc: 0.8621\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.30151 to 0.30019, saving model to best_model.hdf5\n",
            "Epoch 9/20\n",
            "140454/140454 [==============================] - 73s 521us/step - loss: 0.2698 - acc: 0.8773 - val_loss: 0.3014 - val_acc: 0.8604\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.30019\n",
            "Epoch 10/20\n",
            "140454/140454 [==============================] - 72s 514us/step - loss: 0.2663 - acc: 0.8793 - val_loss: 0.3015 - val_acc: 0.8615\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.30019\n",
            "Epoch 11/20\n",
            "140454/140454 [==============================] - 73s 517us/step - loss: 0.2635 - acc: 0.8810 - val_loss: 0.3064 - val_acc: 0.8606\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.30019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8529014d1a239f308ac5f2552088ce2d8ebb8966",
        "id": "xu_ILbwzGZIu",
        "colab_type": "code",
        "outputId": "27a212e3-b551-4054-cd9f-b3b2947ed5d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "cell_type": "code",
      "source": [
        "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred = pred1\n",
        "pred2 = model2.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred2\n",
        "pred3 = model3.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred3\n",
        "pred4 = model4.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred4\n",
        "pred5 = model5.predict(X_test, batch_size = 1024, verbose = 1)\n",
        "pred += pred5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "66292/66292 [==============================] - 5s 80us/step\n",
            "66292/66292 [==============================] - 7s 102us/step\n",
            "66292/66292 [==============================] - 5s 79us/step\n",
            "66292/66292 [==============================] - 5s 77us/step\n",
            "66292/66292 [==============================] - 6s 85us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d58a5b52ea647dab51123ef89878c5355b3d2971",
        "id": "_GvpDKB1GZIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n",
        "sub['Sentiment'] = predictions\n",
        "sub.to_csv(\"blend.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}